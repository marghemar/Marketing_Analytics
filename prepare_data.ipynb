{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from joblib import load, dump\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from load_normalize_dataset.ipynb\n",
      "importing Jupyter notebook from reorder_dataset.ipynb\n",
      "importing Jupyter notebook from create_split_balance_datasets.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Splitting the Dataset:\n",
    "def split_Xy(data, column_y= []):    \n",
    "    if (column_y == []):\n",
    "        column_y=data.columns[-1]\n",
    "    X = data.drop([column_y], axis=1,inplace=False)\n",
    "\n",
    "    y = data[[column_y]].copy()\n",
    "\n",
    "    return [X,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillNormalizedData(X):\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value= 0)\n",
    "    imp_mean.fit(X)\n",
    "\n",
    "    Xfilled=pd.DataFrame(imp_mean.transform(X))\n",
    "    Xfilled.columns=X.columns\n",
    "    Xfilled.index=X.index\n",
    "    return Xfilled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeSomeData(data,column_y=[], categorical_columns = []):\n",
    "\n",
    "   if (column_y == []):\n",
    "    column_y=data.columns[-1]\n",
    "    \n",
    "\n",
    "   [data_mean,data_std] = load('01_import_data_normalization.joblib') \n",
    "\n",
    "   data_norm=data.drop([column_y], axis=1,inplace=False)\n",
    "\n",
    "   data_norm=data.drop(categorical_columns, axis=1,inplace=False)    \n",
    "   data_norm=(data_norm-data_mean[data_norm.columns])/data_std[data_norm.columns]\n",
    "\n",
    "\n",
    "   data_norm[column_y] = data [column_y] \n",
    "   data_norm[categorical_columns] = data [categorical_columns] \n",
    "   data_norm = fillNormalizedData(data_norm)\n",
    "   return data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDFfromCSV(data):\n",
    "    data= data.set_index([data.columns[0],data.columns[1]])\n",
    "    data.index = data.index.set_levels([ datetime.fromisoformat(data.index.levels[1][i]) for i in range(data.index.levels[1].shape[0])], level=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_normalize_data():\n",
    "    \n",
    "    data = pd.read_csv(r'C:\\Users\\margh\\OneDrive\\Documenti\\Adriaclim\\Data\\all_data.csv')\n",
    "\n",
    "    data = createDFfromCSV(data)  \n",
    "    data = data[['SSH', 'MSSH', 'any_damage']]\n",
    "    \n",
    "    train_set, test_set = train_test_split(data, test_size=0.2)\n",
    "    train_set, validation_set = train_test_split(train_set, test_size=0.25)\n",
    "    data_mean = train_set.mean()\n",
    "    data_std = train_set.std()\n",
    "    dump([data_mean,data_std] ,'01_import_data_normalization.joblib')     \n",
    "    \n",
    "    return [normalizeSomeData(data),normalizeSomeData(train_set),normalizeSomeData(validation_set),normalizeSomeData(test_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_training_set(train_set,percentage_rows=100 ):\n",
    "    \n",
    "    n_rows = train_set.shape[0]\n",
    "    tmp=n_rows * percentage_rows/100\n",
    "    new_row_number =math.trunc(tmp )\n",
    "    train_set = train_set.iloc[0:new_row_number,:]\n",
    "\n",
    "    return train_set    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e8336f5aaf8eebef6d48a3b45b08494b66fe67a2a04be680b33896f2163d8b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
